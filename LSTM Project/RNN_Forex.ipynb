{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "RNN_Forex.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnjwvEyn6hvf",
        "colab_type": "code",
        "outputId": "b7ebea32-733a-4a9e-96cd-e210db0ef87b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY39nWLB7v3x",
        "colab_type": "code",
        "outputId": "10b4c5ea-ee69-41cd-fa8a-113d192f74ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yWy7zX9720C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "DRIVE_DIR = '/content/drive/My Drive/Deep/DEXUSEU (2).csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzf_mkj_6hvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numpy.random.seed(7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojldMMgf6hvk",
        "colab_type": "code",
        "outputId": "306f314e-42e1-4cdd-d2c2-1cd15ae125fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import locale\n",
        "import numpy as np\n",
        "locale.setlocale(locale.LC_ALL, 'en_US.UTF8')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'en_US.UTF8'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdO2sREp6hvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataframe = pandas.read_csv(DRIVE_DIR, usecols=[1], engine='python')\n",
        "dataset = dataframe.values\n",
        "#dataset = dataset.astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBs5c7RWLdO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"encoder in DA_RNN.\"\"\"\n",
        "\n",
        "    def __init__(self, T,\n",
        "                 input_size,\n",
        "                 encoder_num_hidden,\n",
        "                 parallel=False):\n",
        "        \"\"\"Initialize an encoder in DA_RNN.\"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "        self.encoder_num_hidden = encoder_num_hidden\n",
        "        self.input_size = input_size\n",
        "        self.parallel = parallel\n",
        "        self.T = T\n",
        "\n",
        "        # Fig 1. Temporal Attention Mechanism: Encoder is LSTM\n",
        "        self.encoder_lstm = nn.LSTM(\n",
        "            input_size=self.input_size,\n",
        "            hidden_size=self.encoder_num_hidden,\n",
        "            num_layers = 1\n",
        "        )\n",
        "\n",
        "        # Construct Input Attention Mechanism via deterministic attention model\n",
        "        # Eq. 8: W_e[h_{t-1}; s_{t-1}] + U_e * x^k\n",
        "        self.encoder_attn = nn.Linear(\n",
        "            in_features=2 * self.encoder_num_hidden + self.T - 1,\n",
        "            out_features=1\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"forward.\n",
        "\n",
        "        Args:\n",
        "            X: input data\n",
        "\n",
        "        \"\"\"\n",
        "        X_tilde = Variable(X.data.new(\n",
        "            X.size(0), self.T - 1, self.input_size).zero_())\n",
        "        X_encoded = Variable(X.data.new(\n",
        "            X.size(0), self.T - 1, self.encoder_num_hidden).zero_())\n",
        "\n",
        "        # Eq. 8, parameters not in nn.Linear but to be learnt\n",
        "        # v_e = torch.nn.Parameter(data=torch.empty(\n",
        "        #     self.input_size, self.T).uniform_(0, 1), requires_grad=True)\n",
        "        # U_e = torch.nn.Parameter(data=torch.empty(\n",
        "        #     self.T, self.T).uniform_(0, 1), requires_grad=True)\n",
        "\n",
        "        # h_n, s_n: initial states with dimention hidden_size\n",
        "        h_n = self._init_states(X)\n",
        "        s_n = self._init_states(X)\n",
        "\n",
        "        for t in range(self.T - 1):\n",
        "            # batch_size * input_size * (2 * hidden_size + T - 1)\n",
        "            x = torch.cat((h_n.repeat(self.input_size, 1, 1).permute(1, 0, 2),\n",
        "                           s_n.repeat(self.input_size, 1, 1).permute(1, 0, 2),\n",
        "                           X.permute(0, 2, 1)), dim=2)\n",
        "\n",
        "            x = self.encoder_attn(\n",
        "                x.view(-1, self.encoder_num_hidden * 2 + self.T - 1))\n",
        "\n",
        "            # get weights by softmax\n",
        "            alpha = F.softmax(x.view(-1, self.input_size))\n",
        "\n",
        "            # get new input for LSTM\n",
        "            x_tilde = torch.mul(alpha, X[:, t, :])\n",
        "\n",
        "            # Fix the warning about non-contiguous memory\n",
        "            # https://discuss.pytorch.org/t/dataparallel-issue-with-flatten-parameter/8282\n",
        "            self.encoder_lstm.flatten_parameters()\n",
        "\n",
        "            # encoder LSTM\n",
        "            _, final_state = self.encoder_lstm(x_tilde.unsqueeze(0), (h_n, s_n))\n",
        "            h_n = final_state[0]\n",
        "            s_n = final_state[1]\n",
        "\n",
        "            X_tilde[:, t, :] = x_tilde\n",
        "            X_encoded[:, t, :] = h_n\n",
        "\n",
        "        return X_tilde, X_encoded\n",
        "\n",
        "    def _init_states(self, X):\n",
        "        \"\"\"Initialize all 0 hidden states and cell states for encoder.\n",
        "\n",
        "        Args:\n",
        "            X\n",
        "\n",
        "        Returns:\n",
        "            initial_hidden_states\n",
        "        \"\"\"\n",
        "        # https://pytorch.org/docs/master/nn.html?#lstm\n",
        "        return Variable(X.data.new(1, X.size(0), self.encoder_num_hidden).zero_())\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"decoder in DA_RNN.\"\"\"\n",
        "\n",
        "    def __init__(self, T, decoder_num_hidden, encoder_num_hidden):\n",
        "        \"\"\"Initialize a decoder in DA_RNN.\"\"\"\n",
        "        super(Decoder, self).__init__()\n",
        "        self.decoder_num_hidden = decoder_num_hidden\n",
        "        self.encoder_num_hidden = encoder_num_hidden\n",
        "        self.T = T\n",
        "\n",
        "        self.attn_layer = nn.Sequential(\n",
        "            nn.Linear(2 * decoder_num_hidden + encoder_num_hidden, encoder_num_hidden),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(encoder_num_hidden, 1)\n",
        "        )\n",
        "        self.lstm_layer = nn.LSTM(\n",
        "            input_size=1,\n",
        "            hidden_size=decoder_num_hidden\n",
        "        )\n",
        "        self.fc = nn.Linear(encoder_num_hidden + 1, 1)\n",
        "        self.fc_final = nn.Linear(decoder_num_hidden + encoder_num_hidden, 1)\n",
        "\n",
        "        self.fc.weight.data.normal_()\n",
        "\n",
        "    def forward(self, X_encoded, y_prev):\n",
        "        \"\"\"forward.\"\"\"\n",
        "        d_n = self._init_states(X_encoded)\n",
        "        c_n = self._init_states(X_encoded)\n",
        "\n",
        "        for t in range(self.T - 1):\n",
        "\n",
        "            x = torch.cat((d_n.repeat(self.T - 1, 1, 1).permute(1, 0, 2),\n",
        "                           c_n.repeat(self.T - 1, 1, 1).permute(1, 0, 2),\n",
        "                           X_encoded), dim=2)\n",
        "\n",
        "            beta = F.softmax(self.attn_layer(\n",
        "                x.view(-1, 2 * self.decoder_num_hidden + self.encoder_num_hidden)).view(-1, self.T - 1))\n",
        "\n",
        "            # Eqn. 14: compute context vector\n",
        "            # batch_size * encoder_hidden_size\n",
        "            context = torch.bmm(beta.unsqueeze(1), X_encoded)[:, 0, :]\n",
        "            if t < self.T - 1:\n",
        "                # Eqn. 15\n",
        "                # batch_size * 1\n",
        "                y_tilde = self.fc(\n",
        "                    torch.cat((context, y_prev[:, t].unsqueeze(1)), dim=1))\n",
        "\n",
        "                # Eqn. 16: LSTM\n",
        "                self.lstm_layer.flatten_parameters()\n",
        "                _, final_states = self.lstm_layer(\n",
        "                    y_tilde.unsqueeze(0), (d_n, c_n))\n",
        "\n",
        "                d_n = final_states[0]  # 1 * batch_size * decoder_num_hidden\n",
        "                c_n = final_states[1]  # 1 * batch_size * decoder_num_hidden\n",
        "\n",
        "        # Eqn. 22: final output\n",
        "        y_pred = self.fc_final(torch.cat((d_n[0], context), dim=1))\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def _init_states(self, X):\n",
        "        \"\"\"Initialize all 0 hidden states and cell states for encoder.\n",
        "\n",
        "        Args:\n",
        "            X\n",
        "        Returns:\n",
        "            initial_hidden_states\n",
        "\n",
        "        \"\"\"\n",
        "        # hidden state and cell state [num_layers*num_directions, batch_size, hidden_size]\n",
        "        # https://pytorch.org/docs/master/nn.html?#lstm\n",
        "        return Variable(X.data.new(1, X.size(0), self.decoder_num_hidden).zero_())\n",
        "from keras.layers import LSTM as DARNN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DA_rnn(nn.Module):\n",
        "    \"\"\"da_rnn.\"\"\"\n",
        "\n",
        "    def __init__(self, X, y, T,\n",
        "                 encoder_num_hidden,\n",
        "                 decoder_num_hidden,\n",
        "                 batch_size,\n",
        "                 learning_rate,\n",
        "                 epochs,\n",
        "                 parallel=False):\n",
        "        \"\"\"da_rnn initialization.\"\"\"\n",
        "        super(DA_rnn, self).__init__()\n",
        "        self.encoder_num_hidden = encoder_num_hidden\n",
        "        self.decoder_num_hidden = decoder_num_hidden\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.parallel = parallel\n",
        "        self.shuffle = False\n",
        "        self.epochs = epochs\n",
        "        self.T = T\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        print(\"==> Use accelerator: \", self.device)\n",
        "\n",
        "        self.Encoder = Encoder(input_size=X.shape[1],\n",
        "                               encoder_num_hidden=encoder_num_hidden,\n",
        "                               T=T).to(self.device)\n",
        "        self.Decoder = Decoder(encoder_num_hidden=encoder_num_hidden,\n",
        "                               decoder_num_hidden=decoder_num_hidden,\n",
        "                               T=T).to(self.device)\n",
        "\n",
        "        # Loss function\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        if self.parallel:\n",
        "            self.encoder = nn.DataParallel(self.encoder)\n",
        "            self.decoder = nn.DataParallel(self.decoder)\n",
        "\n",
        "        self.encoder_optimizer = optim.Adam(params=filter(lambda p: p.requires_grad,\n",
        "                                                          self.Encoder.parameters()),\n",
        "                                            lr=self.learning_rate)\n",
        "        self.decoder_optimizer = optim.Adam(params=filter(lambda p: p.requires_grad,\n",
        "                                                          self.Decoder.parameters()),\n",
        "                                            lr=self.learning_rate)\n",
        "\n",
        "        # Training set\n",
        "        self.train_timesteps = int(self.X.shape[0] * 0.7)\n",
        "        self.y = self.y - np.mean(self.y[:self.train_timesteps])\n",
        "        self.input_size = self.X.shape[1]\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"training process.\"\"\"\n",
        "        iter_per_epoch = int(np.ceil(self.train_timesteps * 1. / self.batch_size))\n",
        "        self.iter_losses = np.zeros(self.epochs * iter_per_epoch)\n",
        "        self.epoch_losses = np.zeros(self.epochs)\n",
        "\n",
        "        n_iter = 0\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            if self.shuffle:\n",
        "                ref_idx = np.random.permutation(self.train_timesteps - self.T)\n",
        "            else:\n",
        "                ref_idx = np.array(range(self.train_timesteps - self.T))\n",
        "\n",
        "            idx = 0\n",
        "\n",
        "            while (idx < self.train_timesteps):\n",
        "                # get the indices of X_train\n",
        "                indices = ref_idx[idx:(idx + self.batch_size)]\n",
        "                # x = np.zeros((self.T - 1, len(indices), self.input_size))\n",
        "                x = np.zeros((len(indices), self.T - 1, self.input_size))\n",
        "                y_prev = np.zeros((len(indices), self.T - 1))\n",
        "                y_gt = self.y[indices + self.T]\n",
        "\n",
        "                # format x into 3D tensor\n",
        "                for bs in range(len(indices)):\n",
        "                    x[bs, :, :] = self.X[indices[bs]:(indices[bs] + self.T - 1), :]\n",
        "                    y_prev[bs, :] = self.y[indices[bs]: (indices[bs] + self.T - 1)]\n",
        "\n",
        "                loss = self.train_forward(x, y_prev, y_gt)\n",
        "                self.iter_losses[int(epoch * iter_per_epoch + idx / self.batch_size)] = loss\n",
        "\n",
        "                idx += self.batch_size\n",
        "                n_iter += 1\n",
        "\n",
        "                if n_iter % 10000 == 0 and n_iter != 0:\n",
        "                    for param_group in self.encoder_optimizer.param_groups:\n",
        "                        param_group['lr'] = param_group['lr'] * 0.9\n",
        "                    for param_group in self.decoder_optimizer.param_groups:\n",
        "                        param_group['lr'] = param_group['lr'] * 0.9\n",
        "\n",
        "                self.epoch_losses[epoch] = np.mean(self.iter_losses[range(\n",
        "                    epoch * iter_per_epoch, (epoch + 1) * iter_per_epoch)])\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(\"Epochs: \", epoch, \" Iterations: \", n_iter,\n",
        "                      \" Loss: \", self.epoch_losses[epoch])\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                y_train_pred = self.test(on_train=True)\n",
        "                y_test_pred = self.test(on_train=False)\n",
        "                y_pred = np.concatenate((y_train_pred, y_test_pred))\n",
        "                plt.ioff()\n",
        "                plt.figure()\n",
        "                plt.plot(range(1, 1 + len(self.y)), self.y, label=\"True\")\n",
        "                plt.plot(range(self.T, len(y_train_pred) + self.T),\n",
        "                         y_train_pred, label='Predicted - Train')\n",
        "                plt.plot(range(self.T + len(y_train_pred), len(self.y) + 1),\n",
        "                         y_test_pred, label='Predicted - Test')\n",
        "                plt.legend(loc='upper left')\n",
        "                plt.show()\n",
        "\n",
        "            # # Save files in last iterations\n",
        "            # if epoch == self.epochs - 1:\n",
        "            #     np.savetxt('../loss.txt', np.array(self.epoch_losses), delimiter=',')\n",
        "            #     np.savetxt('../y_pred.txt',\n",
        "            #                np.array(self.y_pred), delimiter=',')\n",
        "            #     np.savetxt('../y_true.txt',\n",
        "            #                np.array(self.y_true), delimiter=',')\n",
        "\n",
        "    def train_forward(self, X, y_prev, y_gt):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            X:\n",
        "            y_prev:\n",
        "            y_gt: Ground truth label\n",
        "\n",
        "        \"\"\"\n",
        "        # zero gradients\n",
        "        self.encoder_optimizer.zero_grad()\n",
        "        self.decoder_optimizer.zero_grad()\n",
        "\n",
        "        input_weighted, input_encoded = self.Encoder(\n",
        "            Variable(torch.from_numpy(X).type(torch.FloatTensor).to(self.device)))\n",
        "        y_pred = self.Decoder(input_encoded, Variable(\n",
        "            torch.from_numpy(y_prev).type(torch.FloatTensor).to(self.device)))\n",
        "\n",
        "        y_true = Variable(torch.from_numpy(\n",
        "            y_gt).type(torch.FloatTensor).to(self.device))\n",
        "\n",
        "        y_true = y_true.view(-1, 1)\n",
        "        loss = self.criterion(y_pred, y_true)\n",
        "        loss.backward()\n",
        "\n",
        "        self.encoder_optimizer.step()\n",
        "        self.decoder_optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "\n",
        "    def test(self, on_train=False):\n",
        "        \"\"\"test.\"\"\"\n",
        "\n",
        "        if on_train:\n",
        "            y_pred = np.zeros(self.train_timesteps - self.T + 1)\n",
        "        else:\n",
        "            y_pred = np.zeros(self.X.shape[0] - self.train_timesteps)\n",
        "\n",
        "        i = 0\n",
        "        while i < len(y_pred):\n",
        "            batch_idx = np.array(range(len(y_pred)))[i: (i + self.batch_size)]\n",
        "            X = np.zeros((len(batch_idx), self.T - 1, self.X.shape[1]))\n",
        "            y_history = np.zeros((len(batch_idx), self.T - 1))\n",
        "\n",
        "            for j in range(len(batch_idx)):\n",
        "                if on_train:\n",
        "                    X[j, :, :] = self.X[range(\n",
        "                        batch_idx[j], batch_idx[j] + self.T - 1), :]\n",
        "                    y_history[j, :] = self.y[range(\n",
        "                        batch_idx[j], batch_idx[j] + self.T - 1)]\n",
        "                else:\n",
        "                    X[j, :, :] = self.X[range(\n",
        "                        batch_idx[j] + self.train_timesteps - self.T, batch_idx[j] + self.train_timesteps - 1), :]\n",
        "                    y_history[j, :] = self.y[range(\n",
        "                        batch_idx[j] + self.train_timesteps - self.T, batch_idx[j] + self.train_timesteps - 1)]\n",
        "\n",
        "            y_history = Variable(torch.from_numpy(\n",
        "                y_history).type(torch.FloatTensor).to(self.device))\n",
        "            _, input_encoded = self.Encoder(\n",
        "                Variable(torch.from_numpy(X).type(torch.FloatTensor).to(self.device)))\n",
        "            y_pred[i:(i + self.batch_size)] = self.Decoder(input_encoded,\n",
        "                                                           y_history).cpu().data.numpy()[:, 0]\n",
        "            i += self.batch_size\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def read_data(input_path, debug=True):\n",
        "    \"\"\"Read nasdaq stocks data.\n",
        "\n",
        "    Args:\n",
        "        input_path (str): directory to nasdaq dataset.\n",
        "\n",
        "    Returns:\n",
        "        X (np.ndarray): features.\n",
        "        y (np.ndarray): ground truth.\n",
        "\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(input_path, nrows=250 if debug else None)\n",
        "    # X = df.iloc[:, 0:-1].values\n",
        "    X = df.loc[:, [x for x in df.columns.tolist() if x != 'NDX']].as_matrix()\n",
        "    # y = df.iloc[:, -1].values\n",
        "    y = np.array(df.NDX)\n",
        "\n",
        "    return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV5wukwg6hvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = []\n",
        "for x in dataset:\n",
        "    if x == '.' :\n",
        "        pass\n",
        "    else:\n",
        "        X.append([float(x[0])])\n",
        "dataset = np.array(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9i0OP7t6hvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "dataset = scaler.fit_transform(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0leOwG76hvx",
        "colab_type": "code",
        "outputId": "b0d2d86d-c435-42be-979c-ccd867c749c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_size = int(len(dataset) * 0.67)\n",
        "test_size = len(dataset) - train_size\n",
        "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
        "print(len(train), len(test))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3517 1733\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlDN6BwP6hvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(dataset, look_back=1):\n",
        "\tdataX, dataY = [], []\n",
        "\tfor i in range(len(dataset)-look_back-1):\n",
        "\t\ta = dataset[i:(i+look_back), 0]\n",
        "\t\tdataX.append(a)\n",
        "\t\tdataY.append(dataset[i + look_back, 0])\n",
        "\treturn numpy.array(dataX), numpy.array(dataY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUYNMK4b6hv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape into X=t and Y=t+1\n",
        "look_back = 1\n",
        "trainX, trainY = create_dataset(train, look_back)\n",
        "testX, testY = create_dataset(test, look_back)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK1t61u76hv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by5R3ibz6hv8",
        "colab_type": "code",
        "outputId": "ba85a930-3cfa-475a-fe6e-b9b59df3bfec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "# create and fit the LSTM network\n",
        "model = Sequential()\n",
        "model.add(DARNN(4, input_shape=(1, look_back)))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(trainX, trainY, epochs=10, batch_size=1, verbose=2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            " - 9s - loss: 0.0159\n",
            "Epoch 2/10\n",
            " - 8s - loss: 1.4744e-04\n",
            "Epoch 3/10\n",
            " - 8s - loss: 1.4485e-04\n",
            "Epoch 4/10\n",
            " - 8s - loss: 1.4574e-04\n",
            "Epoch 5/10\n",
            " - 8s - loss: 1.4234e-04\n",
            "Epoch 6/10\n",
            " - 8s - loss: 1.4073e-04\n",
            "Epoch 7/10\n",
            " - 8s - loss: 1.3979e-04\n",
            "Epoch 8/10\n",
            " - 8s - loss: 1.3918e-04\n",
            "Epoch 9/10\n",
            " - 8s - loss: 1.3648e-04\n",
            "Epoch 10/10\n",
            " - 8s - loss: 1.4022e-04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7eff3e617b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVIxIjcu6hv-",
        "colab_type": "code",
        "outputId": "77ef1a99-1018-4da7-908e-1648ef307b0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# make predictions\n",
        "trainPredict = model.predict(trainX)\n",
        "testPredict = model.predict(testX)\n",
        "# invert predictions\n",
        "trainPredict = scaler.inverse_transform(trainPredict)\n",
        "trainY = scaler.inverse_transform([trainY])\n",
        "testPredict = scaler.inverse_transform(testPredict)\n",
        "testY = scaler.inverse_transform([testY])\n",
        "# calculate root mean squared error\n",
        "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
        "print('Train Score: %.2f RMSE' % (trainScore))\n",
        "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
        "print('Test Score: %.2f RMSE' % (testScore))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Score: 0.01 RMSE\n",
            "Test Score: 0.01 RMSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZZ64Mty6hwB",
        "colab_type": "code",
        "outputId": "fee7bd47-0b12-4c1f-e729-161d6ecd3e00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "# shift train predictions for plotting\n",
        "trainPredictPlot = numpy.empty_like(dataset)\n",
        "trainPredictPlot[:, :] = numpy.nan\n",
        "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
        "# shift test predictions for plotting\n",
        "testPredictPlot = numpy.empty_like(dataset)\n",
        "testPredictPlot[:, :] = numpy.nan\n",
        "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
        "# plot baseline and predictions\n",
        "plt.plot(scaler.inverse_transform(dataset))\n",
        "plt.plot(trainPredictPlot)\n",
        "plt.plot(testPredictPlot)\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igeGKGp16hwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}